{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4DAEylXnU0S",
        "outputId": "3f18f201-023a-4d83-c2ab-44a6de74724c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.8/146.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.5/664.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet -U langchain langchain-google-genai langchainhub duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K4fVwiIzj1j",
        "outputId": "51de5a53-36f6-4dea-adf6-190e226d04c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.3/59.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yTervuNjprse"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "LANGCHAIN_API_KEY=userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"]=userdata.get(\"GOOGLE_API_KEY\")\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XsEtqbbnqOk7"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"TEST LLM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6fLon1M2w7JI"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "\n",
        "\n",
        "# tools = [DuckDuckGoSearchResults(max_results=1)]\n",
        "tools = [TavilySearchResults(max_results=1)]\n",
        "\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "\n",
        "# Choose the LLM that will drive the agent\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", streaming=True)\n",
        "\n",
        "# Construct the OpenAI Functions agent\n",
        "agent_runnable = create_openai_functions_agent(llm, tools, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HTYTSG-ky6_D"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, List, Union\n",
        "from langchain_core.agents import AgentAction, AgentFinish\n",
        "from langchain_core.messages import BaseMessage\n",
        "import operator\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    # The input string\n",
        "    input: str\n",
        "    # The list of previous messages in the conversation\n",
        "    chat_history: list[BaseMessage]\n",
        "    # The outcome of a given call to the agent\n",
        "    # Needs `None` as a valid type, since this is what this will start as\n",
        "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
        "    # List of actions and corresponding observations\n",
        "    # Here we annotate this with `operator.add` to indicate that operations to\n",
        "    # this state should be ADDED to the existing values (not overwrite it)\n",
        "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8w9vg8-_zaBU"
      },
      "outputs": [],
      "source": [
        "from langchain_core.agents import AgentFinish\n",
        "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
        "\n",
        "# This a helper class we have that is useful for running tools\n",
        "# It takes in an agent action and calls that tool and returns the result\n",
        "tool_executor = ToolExecutor(tools)\n",
        "\n",
        "\n",
        "# Define the agent\n",
        "def run_agent(data):\n",
        "    agent_outcome = agent_runnable.invoke(data)\n",
        "    return {\"agent_outcome\": agent_outcome}\n",
        "\n",
        "\n",
        "# Define the function to execute tools\n",
        "def execute_tools(data):\n",
        "    # Get the most recent agent_outcome - this is the key added in the `agent` above\n",
        "    agent_action = data[\"agent_outcome\"]\n",
        "    output = tool_executor.invoke(agent_action)\n",
        "    return {\"intermediate_steps\": [(agent_action, str(output))]}\n",
        "\n",
        "\n",
        "# Define logic that will be used to determine which conditional edge to go down\n",
        "def should_continue(data):\n",
        "    # If the agent outcome is an AgentFinish, then we return `exit` string\n",
        "    # This will be used when setting up the graph to define the flow\n",
        "    if isinstance(data[\"agent_outcome\"], AgentFinish):\n",
        "        return \"end\"\n",
        "    # Otherwise, an AgentAction is returned\n",
        "    # Here we return `continue` string\n",
        "    # This will be used when setting up the graph to define the flow\n",
        "    else:\n",
        "        return \"continue\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nvwpOxiQzpqQ"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", run_agent)\n",
        "workflow.add_node(\"action\", execute_tools)\n",
        "\n",
        "# Set the entrypoint as `agent`\n",
        "# This means that this node is the first one called\n",
        "workflow.set_entry_point(\"agent\")\n",
        "\n",
        "# We now add a conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    # First, we define the start node. We use `agent`.\n",
        "    # This means these are the edges taken after the `agent` node is called.\n",
        "    \"agent\",\n",
        "    # Next, we pass in the function that will determine which node is called next.\n",
        "    should_continue,\n",
        "    # Finally we pass in a mapping.\n",
        "    # The keys are strings, and the values are other nodes.\n",
        "    # END is a special node marking that the graph should finish.\n",
        "    # What will happen is we will call `should_continue`, and then the output of that\n",
        "    # will be matched against the keys in this mapping.\n",
        "    # Based on which one it matches, that node will then be called.\n",
        "    {\n",
        "        # If `tools`, then we call the tool node.\n",
        "        \"continue\": \"action\",\n",
        "        # Otherwise we finish.\n",
        "        \"end\": END,\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1u4dvwVbzvRo"
      },
      "outputs": [],
      "source": [
        "# We now add a normal edge from `tools` to `agent`.\n",
        "# This means that after `tools` is called, `agent` node is called next.\n",
        "workflow.add_edge(\"action\", \"agent\")\n",
        "\n",
        "# Finally, we compile it!\n",
        "# This compiles it into a LangChain Runnable,\n",
        "# meaning you can use it as you would any other runnable\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5R7xF6qzxmI",
        "outputId": "3b9820a7-3623-4ccb-f660-4975d8273244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'agent_outcome': AgentActionMessageLog(tool='tavily_search_results_json', tool_input={'query': 'what is the weather in sf'}, log=\"\\nInvoking: `tavily_search_results_json` with `{'query': 'what is the weather in sf'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'tavily_search_results_json', 'arguments': '{\"query\": \"what is the weather in sf\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-e1178963-7ea5-4fd9-992a-846383dacc36-0')])}\n",
            "----\n",
            "{'intermediate_steps': [(AgentActionMessageLog(tool='tavily_search_results_json', tool_input={'query': 'what is the weather in sf'}, log=\"\\nInvoking: `tavily_search_results_json` with `{'query': 'what is the weather in sf'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'tavily_search_results_json', 'arguments': '{\"query\": \"what is the weather in sf\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-e1178963-7ea5-4fd9-992a-846383dacc36-0')]), '[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1713705920, \\'localtime\\': \\'2024-04-21 6:25\\'}, \\'current\\': {\\'last_updated_epoch\\': 1713705300, \\'last_updated\\': \\'2024-04-21 06:15\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Partly cloudy\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/116.png\\', \\'code\\': 1003}, \\'wind_mph\\': 11.9, \\'wind_kph\\': 19.1, \\'wind_degree\\': 290, \\'wind_dir\\': \\'WNW\\', \\'pressure_mb\\': 1018.0, \\'pressure_in\\': 30.07, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 89, \\'cloud\\': 25, \\'feelslike_c\\': 12.1, \\'feelslike_f\\': 53.8, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 1.0, \\'gust_mph\\': 16.3, \\'gust_kph\\': 26.3}}\"}]')]}\n",
            "----\n",
            "{'agent_outcome': AgentFinish(return_values={'output': 'Currently, the weather in San Francisco is partly cloudy with a temperature of 53.1 degrees Fahrenheit and humidity of 89%.'}, log='Currently, the weather in San Francisco is partly cloudy with a temperature of 53.1 degrees Fahrenheit and humidity of 89%.')}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"input\": \"what is the weather in sf\", \"chat_history\": []}\n",
        "for s in app.stream(inputs):\n",
        "    print(list(s.values())[0])\n",
        "    print(\"----\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3vQYr35z0cx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
